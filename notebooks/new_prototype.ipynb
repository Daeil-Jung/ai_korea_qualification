{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !python -m pip install numpy scipy matplotlib ipython jupyter pandas sympy nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, datetime, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# memory 관련 이슈를 해결하기 위한 코드\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "# np.random.seed(42)\n",
    "random.seed(12345)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 사전 설정을 위한 세팅\n",
    "epochs = 256\n",
    "image_size = 256\n",
    "batch = 32\n",
    "\n",
    "# Directory check\n",
    "logdir = '../logs'\n",
    "if not os.path.isdir(logdir):\n",
    "    os.mkdir(logdir)\n",
    "    os.mkdir(logdir + \"/disease\")\n",
    "    os.mkdir(logdir + \"/plant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Read dataframe from tsv file\n",
    "train_df = pd.read_csv(\"../data/train/train.tsv\", sep='\\t', header=None)\n",
    "train_df.columns = ['filename', 'plant', 'disease']\n",
    "\n",
    "test_df = pd.read_csv(\"../data/test/test.tsv\", sep='\\t', header=None)\n",
    "test_df.columns = ['filename']\n",
    "\n",
    "# for categorical labels\n",
    "train_df[['plant', 'disease']] = train_df[['plant', 'disease']].astype(str)\n",
    "train_df[\"label\"] = train_df['plant'] + \"_\"+train_df['disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# declare image generator\n",
    "train_image_generator = ImageDataGenerator(rotation_range=5, horizontal_flip=True, validation_split=0.25) # Generator for our training, validation data\n",
    "test_image_generator = ImageDataGenerator() # Generator for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_data_gen = train_image_generator.flow_from_dataframe(dataframe=train_df,\n",
    "                                                           directory=\"../data/train\",\n",
    "                                                           x_col=\"filename\",\n",
    "                                                           y_col=\"label\",\n",
    "                                                           class_mode=\"categorical\",\n",
    "                                                           subset='training',\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(image_size,image_size),\n",
    "                                                           batch_size=batch)\n",
    "\n",
    "valid_data_gen = train_image_generator.flow_from_dataframe(dataframe=train_df,\n",
    "                                                           directory=\"../data/train\",\n",
    "                                                           x_col=\"filename\",\n",
    "                                                           y_col=\"label\",\n",
    "                                                           class_mode=\"categorical\",\n",
    "                                                           subset='validation',\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(image_size,image_size),\n",
    "                                                           batch_size=batch)\n",
    "\n",
    "test_data_gen = test_image_generator.flow_from_dataframe(dataframe=test_df,\n",
    "                                                         directory=\"../data/test\",\n",
    "                                                         x_col=\"filename\",\n",
    "                                                         y_col=None,\n",
    "                                                         class_mode=None,\n",
    "                                                         shuffle=False,\n",
    "                                                         target_size=(image_size,image_size),\n",
    "                                                         batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sample_training_images, _ = next(train_data_gen)\n",
    "\n",
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img/225)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plotImages(sample_training_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Call bottom model\n",
    "inputs = Input(shape = (image_size, image_size, 3))\n",
    "bot_model = EfficientNetB2(weights='imagenet', input_tensor=inputs, classifier_activation=None, include_top=False)\n",
    "# Rebuild top\n",
    "x = GlobalAveragePooling2D(name=\"avg_pool\")(bot_model.output)\n",
    "x = BatchNormalization(name=\"bn_avg_pool\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(176, activation=\"swish\", name=\"top_dense_1\")(x)\n",
    "x = Dense(44, activation=\"swish\", name=\"top_dense_2\")(x)\n",
    "outputs = Dense(20, activation=\"softmax\", name=\"pred\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "model.compile(optimizer=Adam(learning_rate=0.000002),\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "logdir = os.path.join(\"../logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        # The saved model name will include the current epoch.\n",
    "        filepath=\"../model/mymodel_best.h5\",\n",
    "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        min_delta=1e-4,\n",
    "        patience=16,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    TensorBoard(logdir, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_data_gen,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_pred = model.predict(test_data_gen)\n",
    "test_df['label'] = tf.math.argmax(label_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = train_data_gen.class_indices\n",
    "class_indices = dict([(value, key) for key, value in class_indices.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.replace({\"label\": class_indices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"plant\"] = test_df.label.str.split(\"_\").str[0]\n",
    "test_df[\"disease\"] = test_df.label.str.split(\"_\").str[1]\n",
    "test_df = test_df.drop(columns = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"new_test_df.tsv\", sep='\\t', header = False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
